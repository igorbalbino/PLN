{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bd1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow numpy pandas nltk keras\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebd1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# importa dataset\n",
    "mental_health_conversation_dataset = pd.read_csv('./datasets/Mental_Health_FAQ.csv')\n",
    "mental_health_conversation_dataset.drop(columns=['Question_ID'], axis=0, inplace=True)\n",
    "# importa dataset\n",
    "QA_python_dataset = pd.read_csv('./datasets/Dataset_Python_Question_Answer.csv')\n",
    "QA_python_dataset = QA_python_dataset.rename(columns={\"Question\": \"Questions\", \"Answer\": \"Answers\"})\n",
    "#importa dataset\n",
    "conversation_3d_chatbot = pd.read_csv('./datasets/3d-conversation-chatbot.csv')\n",
    "conversation_3d_chatbot.drop(columns=['Unnamed: 0'], axis=0, inplace=True)\n",
    "conversation_3d_chatbot = conversation_3d_chatbot.rename(columns={\"question\": \"Questions\", \"answer\": \"Answers\"})\n",
    "#importa dataset\n",
    "# file = open(\"./datasets/jungleb.txt\", \"r\")\n",
    "# the_jungle_book = file.read()\n",
    "# file.close()\n",
    "# junta datasets\n",
    "df_concated = pd.concat([mental_health_conversation_dataset, QA_python_dataset, conversation_3d_chatbot], ignore_index=True)\n",
    "# tira dados vazios\n",
    "df_concated.dropna()\n",
    "# reseta indexes\n",
    "df_concated = df_concated.reset_index(drop=True)\n",
    "# clona dataset para conservar tratamento\n",
    "df_clone = copy.deepcopy(df_concated)\n",
    "# junta colunas\n",
    "data = {\"text\": []}\n",
    "# data['text'].append(the_jungle_book)\n",
    "for x in range(0, len(df_clone)):\n",
    "    text = \"\".join(df_clone['Questions'][x]+\" \"+df_clone['Answers'][x])\n",
    "    data['text'].append(text) \n",
    "df_merged = pd.DataFrame(data)\n",
    "df_clone2 = df_merged.copy()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "782ce150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "text = df_clone2.text.values\n",
    "joined_text = \"\".join(text)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05b4862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_text = joined_text[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55f45bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52c40e17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'does',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'mental',\n",
       " 'illness',\n",
       " 'mental',\n",
       " 'illnesses',\n",
       " 'are',\n",
       " 'health',\n",
       " 'conditions',\n",
       " 'that',\n",
       " 'disrupt',\n",
       " 'a',\n",
       " 'personâ',\n",
       " 's',\n",
       " 'thoughts',\n",
       " 'emotions',\n",
       " 'relationships',\n",
       " 'and',\n",
       " 'daily',\n",
       " 'functioning',\n",
       " 'they',\n",
       " 'are',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'distress',\n",
       " 'and',\n",
       " 'diminished',\n",
       " 'capacity',\n",
       " 'to',\n",
       " 'engage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ordinary',\n",
       " 'activities',\n",
       " 'of',\n",
       " 'daily',\n",
       " 'life',\n",
       " 'mental',\n",
       " 'illnesses',\n",
       " 'fall',\n",
       " 'along',\n",
       " 'a',\n",
       " 'continuum',\n",
       " 'of',\n",
       " 'severity',\n",
       " 'some',\n",
       " 'are',\n",
       " 'fairly',\n",
       " 'mild',\n",
       " 'and',\n",
       " 'only',\n",
       " 'interfere',\n",
       " 'with',\n",
       " 'some',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'certain',\n",
       " 'phobias',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'spectrum',\n",
       " 'lie',\n",
       " 'serious',\n",
       " 'mental',\n",
       " 'illnesses',\n",
       " 'which',\n",
       " 'result',\n",
       " 'in',\n",
       " 'major',\n",
       " 'functional',\n",
       " 'impairment',\n",
       " 'and',\n",
       " 'interference',\n",
       " 'with',\n",
       " 'daily',\n",
       " 'life',\n",
       " 'these',\n",
       " 'include',\n",
       " 'such',\n",
       " 'disorders',\n",
       " 'as',\n",
       " 'major',\n",
       " 'depression',\n",
       " 'schizophrenia',\n",
       " 'and',\n",
       " 'bipolar',\n",
       " 'disorder',\n",
       " 'and']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(joined_text.lower())\n",
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06d31e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 1530}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = np.unique(tokens)\n",
    "unique_tokens_idx = {\"tokens\": idx for idx, token in enumerate(unique_tokens)}\n",
    "unique_tokens_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3005145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# separando dados para treino. pegamos todas as palavras mas 10 palavras a frente\n",
    "n_words = 10\n",
    "input_words = []\n",
    "next_words = []\n",
    "\n",
    "for i in range(len(tokens)- n_words):\n",
    "    input_words.append(tokens[i:i+n_words])\n",
    "    next_words.append(tokens[i+n_words])\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a77c428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "# para cada exemplo, pegamos n_words a frente e depois o boolean para cada possível próxima palavra\n",
    "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)\n",
    "y = np.zeros((len(next_words), len(unique_tokens)), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a356f1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'what'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, words \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_words):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[1;32m----> 3\u001b[0m         X[i, j, unique_tokens_idx[word]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m     y[i, unique_tokens_idx[next_words[i]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'what'"
     ]
    }
   ],
   "source": [
    "for i, words in enumerate(input_words):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j, unique_tokens_idx[word]] = 1\n",
    "    y[i, unique_tokens_idx[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5cb6c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(LSTM(128, input_shape = (n_words, len(unique_tokens)), return_sequences = True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4f64a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.0707 - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19c03e09090>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = RMSprop(learning_rate=0.01), metrics = [\"accuracy\"])\n",
    "model.fit(X, y, batch_size = 128, epochs = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "807cd20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# salva um arquivo com o modelo para poder ser recuperado depois\n",
    "model.save(\"fourth-try-model.h5\")\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b15cf26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"fourth-try-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aca5b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(input_text, n_best):\n",
    "    input_text = input_text.lower()\n",
    "    X = np.zeros((1, n_words, len(unique_tokens)))\n",
    "    for i, word in enumerate(input_text.split()):\n",
    "        X[0, i, unique_tokens_idx[word]] = 1\n",
    "    predictions = model.predict(X)[0]\n",
    "    return np.argpartition(predictions, -n_best)[-n_best]\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "adbc7875",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'what'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m possible \u001b[38;5;241m=\u001b[39m predict_next_word(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat have I done\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      2\u001b[0m possible\n",
      "Cell \u001b[1;32mIn[66], line 5\u001b[0m, in \u001b[0;36mpredict_next_word\u001b[1;34m(input_text, n_best)\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, n_words, \u001b[38;5;28mlen\u001b[39m(unique_tokens)))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_text\u001b[38;5;241m.\u001b[39msplit()):\n\u001b[1;32m----> 5\u001b[0m     X[\u001b[38;5;241m0\u001b[39m, i, unique_tokens_idx[word]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margpartition(predictions, \u001b[38;5;241m-\u001b[39mn_best)[\u001b[38;5;241m-\u001b[39mn_best]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'what'"
     ]
    }
   ],
   "source": [
    "possible = predict_next_word(\"What have I done\", 5)\n",
    "possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4248a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def generate_text(input_text, text_length, creativity=3):\n",
    "    word_sequence = input_text.split()\n",
    "    current = 0\n",
    "    for _ in range(text_length):\n",
    "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
    "        try:\n",
    "            choice = unique_tokens[random.choice(predict_next_word(sub_sequence, creativity))]\n",
    "        except:\n",
    "            choice = random.choice(unique_tokens)\n",
    "        word_sequence.append(choice)\n",
    "        current += 1\n",
    "    return \" \".join(word_sequence)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ebdd237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what can we do about AI ? had get cannot all toolkits idea nit occur includes knows tired future pleasantly limit big agree recommendations tired share inability straightforward playing recommendations opportunity training therapy reasons spaces severe common untrue delivery 123 effects seems mad hand worked engage cured foreseeable cases months service neurological loneliness unhealthy cards telling avoid actually wouldn addition disrupted includes variety in museums reactions nutrition make performances successful strong member harvard topic either fits impairment physical bc211 behavioral comes vegetables which social management fresh we childâ informed browsing believing research never deal 6 3 sexual disorder pancreas families avoided groceries set pass sick schizophrenia build'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"what can we do about AI ?\", 100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As palavras não são encontradas na bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da respostas completamente desconexas mas acho q isso acontece pois a func predict_next_word não funciona corretamente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
